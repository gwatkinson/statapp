{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('statapp': conda)",
   "metadata": {
    "interpreter": {
     "hash": "378f004e57fe3eca8e623a5d07b582d035b03563d5176b734c6491bec59f748c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Clustering Mapper"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Étapes\n",
    "\n",
    "* Lisser par rapport au temps (B)\n",
    "* Passer au log\n",
    "* Enlever les index\n",
    "* Normaliser\n",
    "* ACP (JB)\n",
    "* km.cover(n = 20, cov = 0.5) (G)\n",
    "* km.map(ACP, data, cover)\n",
    "* Clustering (JB/M)\n",
    "* Créer le graph (M)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importation des modules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Import des modules de bases"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "source": [
    "### Pour normaliser les données\n",
    "\n",
    "Separating out the features\n",
    "\n",
    "    x = df.loc[:, features].values\n",
    "\n",
    "Standardizing the features\n",
    "\n",
    "    x = StandardScaler().fit_transform(x)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "source": [
    "### Pour faire l'ACP\n",
    "\n",
    "Initialise la classe\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "\n",
    "Fit le modèle\n",
    "\n",
    "    principalComponents = pca.fit_transform(x)\n",
    "\n",
    "Transforme en df pandas\n",
    "\n",
    "    principalDf = pd.DataFrame(data = principalComponents\n",
    "                , columns = ['principal component 1', 'principal component 2'])\n",
    "    finalDf = pd.concat([df[index]], principalDf, axis = 1)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "source": [
    "### Pour faire le clustering\n",
    "\n",
    "En utilisant sklearn :\n",
    "\n",
    "    model = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='single')\n",
    "    model.fit(X)\n",
    "    labels = model.labels_"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "source": [
    "En utilisant scipy :\n",
    "\n",
    "    link = sch.linkage(y, method='single\", metric='...')\n",
    "    dendrogram = sch.dendrogram(link)\n",
    "\n",
    "Voir https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch "
   ]
  },
  {
   "source": [
    "### Keppler Mapper"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kmapper as km\n",
    "from kmapper import jupyter # Creates custom CSS full-size Jupyter screen"
   ]
  },
  {
   "source": [
    "## Chargement des données"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "data_firm_level = pd.read_stata(r\"..\\Data\\Firm_patent\\data_firm_level.dta\")\n",
    "data_patent_level = pd.read_stata(r\"..\\Data\\Patent_level_data\\data_patent_level.dta\")\n",
    "cites = pd.read_stata(r\"..\\Data\\Patent_level_data\\USPatent_1926-2010\\cites\\cites.dta\")\n",
    "firm_innovation_v2 = pd.read_stata(r\"..\\Data\\Patent_level_data\\USPatent_1926-2010\\firm_innovation\\firm_innovation_v2.dta\")\n",
    "patents_xi = pd.read_stata(r\"..\\Data\\Patent_level_data\\USPatent_1926-2010\\patents_xi\\patents_xi.dta\")\n",
    "patent_values = pd.read_stata(r\"..\\Data\\Patent_level_data\\Patent_CRSP_match_1929-2017\\patent_values\\patent_values.dta\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## Utilisation de la base merged"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents_firm_merge = pd.read_stata(r\"..\\Data\\Firm_patent\\patents_firm_merge.dta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents_firm_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_df = patents_firm_merge\n",
    "for col in [\"fdate\", \"idate\", \"pdate\"]:\n",
    "    datetime_df[col] = pd.to_datetime(patents_firm_merge[col], infer_datetime_format=True, errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_df.count()/len(datetime_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = datetime_df.dropna(subset=['xi', 'ncites', 'tcw', 'tsm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.count()/len(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.groupby(\"permno\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.drop([\"fdate\", \"pdate\", \"year\", \"_merge\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [c for c in patents_firm_merge.columns if c not in [\"index\", \"patnum\", \"fdate\", \"idate\", \"pdate\", \"permno\", \"year\", \"Npats\", \"_merge\", \"patent_class\", \"subclass\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = patents_firm_merge[feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.astype({\"patent_class\": float, \"subclass\": float})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents_firm_merge.iloc[20000:20010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents_firm_merge.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents_firm_merge.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents_firm_merge[\"permno\"].value_counts()"
   ]
  },
  {
   "source": [
    "# Some sample data\n",
    "from sklearn import datasets\n",
    "data, labels = datasets.make_circles(n_samples=5000, noise=0.03, factor=0.3)\n",
    "\n",
    "# Initialize\n",
    "mapper = km.KeplerMapper(verbose=1)\n",
    "\n",
    "# Fit to and transform the data\n",
    "projected_data = mapper.fit_transform(data, projection=[0,1]) # X-Y axis\n",
    "\n",
    "# Create dictionary called 'graph' with nodes, edges and meta-information\n",
    "graph = mapper.map(projected_data, data)\n",
    "\n",
    "# Visualize it\n",
    "html = mapper.visualize(graph, path_html=\"make_circles_keplermapper_output.html\",\n",
    "                 title=\"make_circles(n_samples=5000, noise=0.03, factor=0.3)\")\n",
    "\n",
    "# Inline display\n",
    "# jupyter.display(path_html=\"http://mlwave.github.io/tda/word2vec-gender-bias.html\")\n",
    "jupyter.display(path_html=\"make_circles_keplermapper_output.html\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#projected_data\n",
    "from sklearn.decomposition import PCA\n",
    "x=df.values\n",
    "#preprocessing avant PCA\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scale=scaler.fit(X)\n",
    "X_scaled=scaler.transform(X)\n",
    "X_scaled_df=pd.DataFrame(X_scaled,columns=X.columns)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "PC = pca.fit_transform(X_scaled_df)\n",
    "p_Df = pd.DataFrame(data = PC\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "p_Df.head()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "projected_data = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "projected_dataframe = pd.concat([projected_data, df[['target']]], axis = 1)\n",
    "\n",
    "fig = plt.figure(figsize = (1000,1000))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "colors = ['r', 'g', 'b']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['target'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  }
 ]
}